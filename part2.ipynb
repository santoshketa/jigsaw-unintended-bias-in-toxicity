{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suUbEv9sKUbe"
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRLW2HBGiFKh"
   },
   "outputs": [],
   "source": [
    "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/12500/824022/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1575169444&Signature=eVeXAucRcVmn6Ra14LLaGqhnIYCB1eHkVVgCuCq3PALafECs71RAZOt0Q323rNjpKFq%2BVn62jsi9qzJqTVkFyZerXLoi7cCbvdlCiDA39Er84Dd8ckj549ncSE3PLqDIg%2F%2F4Mc5GOCM%2B09sHddQDllUb8IE2tNzvR6cw7WWO94tliwrw6S9adDpdSN%2BxnCYBPLKSK%2FRNV9kZXbQkPEoc%2Fed9rd61msFIY%2B4mb74ot6r07XSIj%2FaJ2Bgiuiduc94BHbviGWC3ISMVeKOPYGqco1x2lTjY8c5rbrMgyMN2CuEzIBsXV7Ya7XqJbfGwXbPVSlmsZs7O3aYeYNzlu7UtGg%3D%3D\" -O \"train.csv.zip\" -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXE8Mz3zivX1"
   },
   "outputs": [],
   "source": [
    "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/12500/824022/test.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1575169411&Signature=jtOtKUkUKARCIaIjgVMYX1dQTuIvYqqCjcL2mNVVw%2BvhOzmeavb%2FTCYYT7X52d2Ph3ssh6YUEqY%2BQuGuI0tnOXXSA177RaaF9DBTXoFoKXiznFUx0UGwI0B2rdKim%2BKJC0JhxEyiVbMu28wlLoU59P0wQ4xK6sL0ZIKgc00%2Bmca7sWTcCgAaLHb9k2CUa8HZdPKX2fvZDdEtz2CFy0klqUK2lyc4dPCZCgGTSUVYLkR%2BWIh%2F%2FRvN9vNWEr6zmrhuGG3tplDNy8fjGxLIPcNKjZfOWJiXw4Kqoy7dJgUxscIyi0ySBJKIDGpCuHbOJpjYrvVOZzrIjMCZcR4zOj9NbQ%3D%3D\" -O \"test.csv.zip\" -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lby3BT38olJY"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3456,
     "status": "ok",
     "timestamp": 1575167753590,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "1mB2zGYEn5S1",
    "outputId": "b3d811df-a882-4551-9f64-858a6c25aa7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout,CuDNNLSTM,Flatten,Layer,GlobalAveragePooling1D,add,Conv1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from kerafrom google.colab import files\n",
    "import os.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from google.colab import files\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19270,
     "status": "ok",
     "timestamp": 1575167769786,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "inTe02f01QmQ",
    "outputId": "873a55cc-5837-4fad-9def-7439ac4f72a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data: (1804874, 45)\n"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "df_train = pd.read_csv('train.csv.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "df_test = pd.read_csv('test.csv.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "print(\"shape of train data:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSjrJA8B1X1H"
   },
   "outputs": [],
   "source": [
    "# Assigning class label based on target variable\n",
    "def label(target):                           \n",
    "    return 0 if target < 0.5 else 1\n",
    "        \n",
    "df_train['class'] = df_train.apply(lambda x: label(x['target']), axis= 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__yzSQ9H-9sa"
   },
   "source": [
    "# Splitting data into Train-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35227,
     "status": "ok",
     "timestamp": 1575167805036,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "ZortJx2ZoZmE",
    "outputId": "dcf0aa47-cdc7-4d2a-f312-54713fde5606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points in train 1624386 and cv 180488\n"
     ]
    }
   ],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(df_train, df_train['class'], test_size=0.1)\n",
    "print('Data points in train {} and cv {}'. format(X_train.shape[0],X_cv.shape[0]))\n",
    "\n",
    "X_train1 = X_train['comment_text'].astype(str)\n",
    "X_cv1 = X_cv['comment_text'].astype(str)\n",
    "X_test = df_test['comment_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfiTZxe65DVk"
   },
   "outputs": [],
   "source": [
    "num_words = 35000 # using top 35000 frequent words\n",
    "max_len = 100 \n",
    "emb_size = 128\n",
    "#Tokenizer converts words to number based their frequency of occurance.\n",
    "t = Tokenizer(num_words = num_words)\n",
    "t.fit_on_texts(list(X_train1))\n",
    "# converting each comment to sequence\n",
    "X_train1 = t.texts_to_sequences(X_train1)\n",
    "X_cv1=t.texts_to_sequences(X_cv1)\n",
    "test = t.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding with max lenght of 100\n",
    "X_train1 = sequence.pad_sequences(X_train1, maxlen=max_len)\n",
    "X_cv1 = sequence.pad_sequences(X_cv1, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZY3QOEc5DYy"
   },
   "outputs": [],
   "source": [
    "# function for Standard auc\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wB3vyRm3_nOg"
   },
   "source": [
    "# Custom AUC for unintended bias in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UolejJcbLgi9"
   },
   "outputs": [],
   "source": [
    "# code for calculating Custom auc taken from kaggle\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNp4sb00Lmyo"
   },
   "outputs": [],
   "source": [
    "# overall auc adding bias to each subgroup\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2CtylNrvwvB"
   },
   "outputs": [],
   "source": [
    "# converting label and identity colums to boolean values\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "X_cv = convert_dataframe_to_bool(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaLylGfGPY_Y"
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(y_train)\n",
    "cv_y = to_categorical(y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MTrFY0L0JZx"
   },
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4996,
     "status": "ok",
     "timestamp": 1575087070077,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "ZkJRZCNJ5Dg7",
    "outputId": "ad3b9ad0-d6a6-4ae2-a616-a8d60c3dbc17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 100, 128)          4480000   \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 100, 100)          71600     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 4,556,752\n",
      "Trainable params: 4,556,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://realpython.com/python-keras-text-classification/\n",
    "inp = Input(shape = (max_len, ))\n",
    "layer = Embedding(num_words, emb_size)(inp)\n",
    "layer = Bidirectional(LSTM(50, return_sequences=True, recurrent_dropout=0.15))(layer)\n",
    "layer = GlobalMaxPool1D()(layer)\n",
    "layer = Dense(50, activation='relu')(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(2, activation='sigmoid')(layer)\n",
    "\n",
    "model = Model(inputs=inp, outputs=layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2569464,
     "status": "ok",
     "timestamp": 1575093718003,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "ze_72_vw5Dj0",
    "outputId": "e7898810-e27a-44c9-c3b4-51f59144f0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/5\n",
      "1624386/1624386 [==============================] - 512s 315us/step - loss: 0.1281 - auc: 0.9849 - val_loss: 0.1322 - val_auc: 0.9862\n",
      "Epoch 2/5\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1183 - auc: 0.9872 - val_loss: 0.1323 - val_auc: 0.9879\n",
      "Epoch 3/5\n",
      "1624386/1624386 [==============================] - 511s 314us/step - loss: 0.1065 - auc: 0.9886 - val_loss: 0.1396 - val_auc: 0.9891\n",
      "Epoch 4/5\n",
      "1624386/1624386 [==============================] - 510s 314us/step - loss: 0.0920 - auc: 0.9897 - val_loss: 0.1544 - val_auc: 0.9901\n",
      "Epoch 5/5\n",
      "1624386/1624386 [==============================] - 521s 321us/step - loss: 0.0761 - auc: 0.9907 - val_loss: 0.1868 - val_auc: 0.9911\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train1, train_y, batch_size=1024, epochs=5, validation_data=(X_cv1,cv_y),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16874,
     "status": "ok",
     "timestamp": 1575094425005,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "D2J7ZAdSarQu",
    "outputId": "1d1c9c5e-d355-4f2f-8844-d68ba210397b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1436</td>\n",
       "      <td>0.737179</td>\n",
       "      <td>0.775611</td>\n",
       "      <td>0.917753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2475</td>\n",
       "      <td>0.764191</td>\n",
       "      <td>0.778985</td>\n",
       "      <td>0.928894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1083</td>\n",
       "      <td>0.767462</td>\n",
       "      <td>0.787029</td>\n",
       "      <td>0.924113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2076</td>\n",
       "      <td>0.785124</td>\n",
       "      <td>0.809262</td>\n",
       "      <td>0.924849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>801</td>\n",
       "      <td>0.792037</td>\n",
       "      <td>0.852692</td>\n",
       "      <td>0.888648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5382</td>\n",
       "      <td>0.859447</td>\n",
       "      <td>0.880362</td>\n",
       "      <td>0.909955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4134</td>\n",
       "      <td>0.859748</td>\n",
       "      <td>0.907749</td>\n",
       "      <td>0.880346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4504</td>\n",
       "      <td>0.860599</td>\n",
       "      <td>0.868936</td>\n",
       "      <td>0.921170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>489</td>\n",
       "      <td>0.873470</td>\n",
       "      <td>0.846856</td>\n",
       "      <td>0.935578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  ...  bpsn_auc  bnsp_auc\n",
       "6                          black           1436  ...  0.775611  0.917753\n",
       "7                          white           2475  ...  0.778985  0.928894\n",
       "2      homosexual_gay_or_lesbian           1083  ...  0.787029  0.924113\n",
       "5                         muslim           2076  ...  0.809262  0.924849\n",
       "4                         jewish            801  ...  0.852692  0.888648\n",
       "1                         female           5382  ...  0.880362  0.909955\n",
       "3                      christian           4134  ...  0.907749  0.880346\n",
       "0                           male           4504  ...  0.868936  0.921170\n",
       "8  psychiatric_or_mental_illness            489  ...  0.846856  0.935578\n",
       "\n",
       "[9 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOXICITY_COLUMN = 'target'\n",
    "MODEL_NAME = 'model1'\n",
    "X_cv[MODEL_NAME] = model.predict(X_cv1)[:,1]\n",
    "bias_metrics_df = compute_bias_metrics_for_model(X_cv, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "bias_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3541,
     "status": "ok",
     "timestamp": 1575094568856,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "ZvK1HES1upeD",
    "outputId": "b7255b83-2612-47c4-c224-70b7246fd48b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8658840551094565"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_metric(bias_metrics_df, calculate_overall_auc(X_cv, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-SUquzUZFq3"
   },
   "source": [
    "# Observation:\n",
    "1.We can see stardard AUC on CV data is 0.99 which is due to class imbalance .\n",
    "\n",
    "2.Due to adding weights to each identity columns ,the overall weighted average AUC is less as compared to standard AUC.\n",
    "\n",
    "3.From metric table above jewish subgroup has comparetively less AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egmnjMPb6XCS"
   },
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6778,
     "status": "ok",
     "timestamp": 1575094812331,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "NHYR9XZb0Vlf",
    "outputId": "c4d91936-5f27-4d6f-d117-9e637729a612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 100, 128)          4480000   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 100, 100)          71600     \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 100, 100)          60400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,620,610\n",
      "Trainable params: 4,620,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://realpython.com/python-keras-text-classification/\n",
    "inp = Input(shape = (max_len, ))\n",
    "layer = Embedding(num_words, emb_size)(inp)\n",
    "layer = Bidirectional(LSTM(50,return_sequences=True,  recurrent_dropout=0.15))(layer)\n",
    "layer = Bidirectional(LSTM(50,return_sequences=True ))(layer)\n",
    "layer = GlobalMaxPool1D()(layer)\n",
    "layer = Dense(64, activation='relu')(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(32, activation='relu')(layer)\n",
    "layer = Dense(2, activation='sigmoid')(layer)\n",
    "\n",
    "model2 = Model(inputs=inp, outputs=layer)\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2895324,
     "status": "ok",
     "timestamp": 1575097703760,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "Z4-z7fYk9TA0",
    "outputId": "b6b277f2-5879-42f0-8435-29957a0ce4c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/3\n",
      "1624386/1624386 [==============================] - 974s 600us/step - loss: 0.1582 - auc: 0.9678 - val_loss: 0.1309 - val_auc: 0.9840\n",
      "Epoch 2/3\n",
      "1624386/1624386 [==============================] - 959s 590us/step - loss: 0.1252 - auc: 0.9861 - val_loss: 0.1291 - val_auc: 0.9872\n",
      "Epoch 3/3\n",
      "1624386/1624386 [==============================] - 956s 588us/step - loss: 0.1149 - auc: 0.9881 - val_loss: 0.1314 - val_auc: 0.9887\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model2.fit(X_train1, train_y, batch_size=1024, epochs=3, validation_data=(X_cv1,cv_y),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1168874,
     "status": "ok",
     "timestamp": 1575098924130,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "Kj3DRiXFJCHT",
    "outputId": "f4650699-53f5-4fd2-a96b-a929d4277783"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1436</td>\n",
       "      <td>0.777643</td>\n",
       "      <td>0.777804</td>\n",
       "      <td>0.964324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1083</td>\n",
       "      <td>0.795574</td>\n",
       "      <td>0.804998</td>\n",
       "      <td>0.960011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2475</td>\n",
       "      <td>0.810180</td>\n",
       "      <td>0.797744</td>\n",
       "      <td>0.964887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2076</td>\n",
       "      <td>0.828485</td>\n",
       "      <td>0.841605</td>\n",
       "      <td>0.956264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>801</td>\n",
       "      <td>0.863732</td>\n",
       "      <td>0.866221</td>\n",
       "      <td>0.954987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>489</td>\n",
       "      <td>0.890720</td>\n",
       "      <td>0.873463</td>\n",
       "      <td>0.959006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5382</td>\n",
       "      <td>0.899197</td>\n",
       "      <td>0.906251</td>\n",
       "      <td>0.949764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4504</td>\n",
       "      <td>0.899653</td>\n",
       "      <td>0.892357</td>\n",
       "      <td>0.958189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4134</td>\n",
       "      <td>0.906246</td>\n",
       "      <td>0.927169</td>\n",
       "      <td>0.935674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  ...  bpsn_auc  bnsp_auc\n",
       "6                          black           1436  ...  0.777804  0.964324\n",
       "2      homosexual_gay_or_lesbian           1083  ...  0.804998  0.960011\n",
       "7                          white           2475  ...  0.797744  0.964887\n",
       "5                         muslim           2076  ...  0.841605  0.956264\n",
       "4                         jewish            801  ...  0.866221  0.954987\n",
       "8  psychiatric_or_mental_illness            489  ...  0.873463  0.959006\n",
       "1                         female           5382  ...  0.906251  0.949764\n",
       "0                           male           4504  ...  0.892357  0.958189\n",
       "3                      christian           4134  ...  0.927169  0.935674\n",
       "\n",
       "[9 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'model2'\n",
    "X_cv[MODEL_NAME] = model2.predict(X_cv1)[:,1]\n",
    "bias_metrics_df = compute_bias_metrics_for_model(X_cv, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "bias_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1575099359113,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "AB1SEpVkPKez",
    "outputId": "6258d635-c3d8-4728-ece9-57e7bba407b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8988726969000456"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_metric(bias_metrics_df, calculate_overall_auc(X_cv, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW_m9qOLcQs8"
   },
   "source": [
    "# Observation:\n",
    "\n",
    "1.Adding another bi-lstm layer and slightly tweeking model1 architecture has sifincantly improved overall AUC score in model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_aNZOOnymXg"
   },
   "source": [
    "# MODEL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHhPMAtkn5YC"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150 \n",
    "MAX_NUM_WORDS = 200000 # changed max frequent words to 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34285,
     "status": "ok",
     "timestamp": 1575167852089,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "ijYA2u10n5bC",
    "outputId": "92980aaa-fb23-49d6-ad21-500a993176eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# loading word vectors\n",
    "embeddings_index = {}\n",
    "with open('/content/drive/My Drive/glove.6B.300d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 166163,
     "status": "ok",
     "timestamp": 1575168018259,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "L1-Vr8btn5dp",
    "outputId": "c73c1285-dbbe-41fa-fabc-8b689add5a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 397708 unique tokens.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "X_train = df_train['comment_text'].astype(str)\n",
    "X_test = df_test['comment_text'].astype(str)\n",
    "#Tokenizer converts words to number based their frequency of occurance.\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIVWwNnbn5g3"
   },
   "outputs": [],
   "source": [
    "# converting word to sequencrs\n",
    "X = tokenizer.texts_to_sequences(X_train)\n",
    "test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBcu3u5en5qc"
   },
   "outputs": [],
   "source": [
    "# zero padding\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH )\n",
    "X_test = sequence.pad_sequences(test, maxlen=MAX_SEQUENCE_LENGTH )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNK9LNVCn5uS"
   },
   "outputs": [],
   "source": [
    "def label(target):\n",
    "    return 0 if target < 0.5 else 1\n",
    "        \n",
    "df_train['class'] = df_train.apply(lambda x: label(x['target']), axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIPF_4upfLAt"
   },
   "source": [
    "## Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MzFcr-wn5yn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# credits: https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\n",
    "from keras import backend as K\n",
    "from keras import initializers,regularizers,constraints\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving weights for identity words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLw_SamtCiGl"
   },
   "outputs": [],
   "source": [
    "# Idea taken from 2nd place solution\n",
    "y_aux_train = df_train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values # subgroup columns\n",
    "identity_columns = [\n",
    "        'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "        'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# calculating weights for each data points. This will be used during loss calculation.\n",
    "# Each of the 4 sections can contribute 25% to the overall weight.\n",
    "\n",
    "# Overall\n",
    "weights = np.ones((len(df_train),)) / 4\n",
    "\n",
    "# Subgroup\n",
    "weights += (df_train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (df_train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (df_train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (df_train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (df_train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "loss_weight = 1.0 / weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3098,
     "status": "ok",
     "timestamp": 1575180214801,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "ttzc3INRuNmq",
    "outputId": "91c59cc0-5297-414b-df5f-f27e459ac3eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 150, 300)     60000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 150, 512)     1140736     embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 150, 256)     657408      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_2 (Atten (None, 256)          66048       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           4112        attention_with_context_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            17          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            102         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 61,868,423\n",
      "Trainable params: 1,868,423\n",
      "Non-trainable params: 60,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(150,))\n",
    "x = Embedding(num_words,\n",
    "                 EMBEDDING_DIM,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=MAX_SEQUENCE_LENGTH, # load pre-trained word embeddings into an Embedding layer\n",
    "                trainable=False)(inp)            # note that we set trainable = False so as to keep the embeddings fixed\n",
    "\n",
    "x = Bidirectional(LSTM(256,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "at = AttentionWithContext()(x)\n",
    "x = Dense(16, activation=\"relu\")(at)\n",
    "ide_output = Dense(6, activation=\"sigmoid\")(x) # predicting subgroups\n",
    "out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inp], outputs=[out, ide_output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'],loss_weights=[loss_weight, 1.0] )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 742790,
     "status": "ok",
     "timestamp": 1574657503904,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "iSrjK6ptsHN1",
    "outputId": "ad7fd625-fae8-4e5a-952c-eaf3c82a9059",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/4\n",
      "1443899/1443899 [==============================] - 1267s 878us/step - loss: 0.6000 - dense_3_loss: 0.1498 - dense_2_loss: 0.1192 - dense_3_acc: 0.9446 - dense_2_acc: 0.8535 - val_loss: 0.5538 - val_dense_3_loss: 0.1375 - val_dense_2_loss: 0.1124 - val_dense_3_acc: 0.9463 - val_dense_2_acc: 0.8490\n",
      "Epoch 2/4\n",
      "1443899/1443899 [==============================] - 1246s 863us/step - loss: 0.5192 - dense_3_loss: 0.1284 - dense_2_loss: 0.1073 - dense_3_acc: 0.9505 - dense_2_acc: 0.8563 - val_loss: 0.5317 - val_dense_3_loss: 0.1315 - val_dense_2_loss: 0.1097 - val_dense_3_acc: 0.9481 - val_dense_2_acc: 0.8491\n",
      "Epoch 3/4\n",
      "1443899/1443899 [==============================] - 1245s 862us/step - loss: 0.4997 - dense_3_loss: 0.1228 - dense_2_loss: 0.1055 - dense_3_acc: 0.9522 - dense_2_acc: 0.8564 - val_loss: 0.5193 - val_dense_3_loss: 0.1280 - val_dense_2_loss: 0.1085 - val_dense_3_acc: 0.9496 - val_dense_2_acc: 0.8491\n",
      "Epoch 4/4\n",
      "1443899/1443899 [==============================] - 1246s 863us/step - loss: 0.4873 - dense_3_loss: 0.1192 - dense_2_loss: 0.1047 - dense_3_acc: 0.9534 - dense_2_acc: 0.8564 - val_loss: 0.5232 - val_dense_3_loss: 0.1293 - val_dense_2_loss: 0.1083 - val_dense_3_acc: 0.9481 - val_dense_2_acc: 0.8490\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X,[ df_train['class'],y_aux_train], batch_size=730, epochs=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvQvAFhFBh9-"
   },
   "source": [
    "1.Overall AUC on test data obtained was 0.92172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4ztFtJEsHRA"
   },
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "sub = pd.read_csv('/content/drive/My Drive/sample_submission.csv')\n",
    "sub['prediction'] = y_test[0]\n",
    "sub.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdYGvyGoOBBm"
   },
   "source": [
    " ## Procedure Followed:\n",
    "1.Built three models with Bi directional Lstm with different architectures .\n",
    "\n",
    "2.For first two models just trained them without any word embedding matrix and without any weights to background and subgroups.\n",
    "\n",
    "3.Both first two models were not crossing the custom AUC score of 0.90 even while increasing the epochs.\n",
    "\n",
    "4.Built another model with more complex aechitecture which includes adding weights to subgroups and backgrounds , adding word embedding matrix and with state of the art attention layer which helped to improve AUC 0.922\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1575189522311,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "rCJT-FIAsHf0",
    "outputId": "3e13e7e1-0b2b-46d4-80b7-38faade0abbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------------------------------+----------------+------------------+\n",
      "|  Model  |                        architecture                       |  custom cv AUC | custom test AUC  |\n",
      "+---------+-----------------------------------------------------------+----------------+------------------+\n",
      "| MODEL 1 |          1-Bi lstm  with no word embedding matrix         |     0.8658     |        --        |\n",
      "| MODEL 2 |          2-Bi lstm  with no word embedding matrix         |     0.8988     |        --        |\n",
      "| MODEL 3 | 2-Bi lstm  with word embedding matrix and attention layer |       --       |      0.9217      |\n",
      "+---------+-----------------------------------------------------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "conclusion= PrettyTable()\n",
    "conclusion.field_names = [ \"Model\", \"architecture\", \" custom cv AUC\",'custom test AUC ']\n",
    "\n",
    "conclusion.add_row([\"MODEL 1\", \"1-Bi lstm  with no word embedding matrix\", 0.8658,'--'])\n",
    "conclusion.add_row([\"MODEL 2\", \"2-Bi lstm  with no word embedding matrix\", 0.8988, '--'])\n",
    "conclusion.add_row([\"MODEL 3\", \"2-Bi lstm  with word embedding matrix and attention layer\",  '--',0.9217])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NXN7ewd5XHx"
   },
   "source": [
    "# Conclusion:\n",
    "1.Model 1 and 2 failed to distinguish between the classes even though using bi- directional LSTM because it is unable to capture long term dependency of words there by misinterpreting identity words to be toxic.\n",
    "\n",
    "2.By including attention layer in model3  helped to remember long term dependencies of words there by understanding context of sentences which increased AUC by significant margin.\n",
    "\n",
    "3.Glove vectors of words and adding loss weights to each identity groups is key booster for AUC.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Copy of Untitled9.ipynb",
   "provenance": [
    {
     "file_id": "18UhQBeKc05DLmSM-7qMATFJodKMvCwuJ",
     "timestamp": 1575195672434
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
